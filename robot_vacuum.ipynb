{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents: Reflex-Based Agents for the Vacuum-cleaner World\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undergrads 100 / Graduate students 110\n",
    "\n",
    "Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook as a PDF file. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment you will implement a simulator environment for an automatic vacuum cleaner robot, a set of different reflex-based agent programs, and perform a comparison study for cleaning a single room. Focus on the __cleaning phase__ which starts when the robot is activated and ends when the last dirty square in the room has been cleaned. Someone else will take care of the agent program needed to navigate back to the charging station after the room is clean.\n",
    "\n",
    "## PEAS description of the cleaning phase\n",
    "\n",
    "__Performance Measure:__ Each action costs 1 energy unit. The performance is measured as the sum of the energy units used to clean the whole room.\n",
    "\n",
    "__Environment:__ A room with $n \\times n$ squares where $n = 5$. Dirt is randomly placed on each square with probability $p = 0.2$. For simplicity, you can assume that the agent knows the size and the layout of the room (i.e., it knows $n$). To start, the agent is placed on a random square.\n",
    "\n",
    "__Actuators:__ The agent can clean the current square (action `suck`) or move to an adjacent square by going `north`, `east`, `south`, or `west`.\n",
    "\n",
    "__Sensors:__ Four bumper sensors, one for north, east, south, and west; a dirt sensor reporting dirt in the current square.  \n",
    "\n",
    "\n",
    "## The agent program for a simple randomized agent\n",
    "\n",
    "The agent program is a function that gets sensor information (the current percepts) as the arguments. The arguments are:\n",
    "\n",
    "* A dictionary with boolean entries for the for bumper sensors `north`, `east`, `west`, `south`. E.g., if the agent is on the north-west corner, `bumpers` will be `{\"north\" : True, \"east\" : False, \"south\" : False, \"west\" : True}`.\n",
    "* The dirt sensor produces a boolean.\n",
    "\n",
    "The agent returns the chosen action as a string.\n",
    "\n",
    "Here is an example implementation for the agent program of a simple randomized agent:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "def simple_randomized_agent(bumpers, dirty):\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'south'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define percepts (current location is NW corner and it is dirty)\n",
    "bumpers = {\"north\" : True, \"east\" : False, \"south\" : False, \"west\" : True}\n",
    "dirty = True\n",
    "\n",
    "# call agent program function with percepts and it returns an action\n",
    "simple_randomized_agent(bumpers, dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ This is not a rational intelligent agent. It ignores its sensors and may bump into a wall repeatedly or not clean a dirty square. You will be asked to implement rational agents below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple environment example\n",
    "\n",
    "We implement a simple simulation environment that supplies the agent with its percepts.\n",
    "The simple environment is infinite in size (bumpers are always `False`) and every square is always dirty, even if the agent cleans it. The environment function returns a performance measure which is here the number of cleaned squares (since the room is infinite and all squares are constantly dirty, the agent can never clean the whole room as required in the PEAS description above). The energy budget of the agent is specified as `max_steps`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_environment(agent, max_steps, verbose = True):\n",
    "    num_cleaned = 0\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        dirty = True\n",
    "        bumpers = {\"north\" : False, \"south\" : False, \"west\" : False, \"east\" : False}\n",
    "\n",
    "        action = agent(bumpers, dirty)\n",
    "        if (verbose): print(\"step\", i , \"- action:\", action) \n",
    "        \n",
    "        if (action == \"suck\"): \n",
    "            num_cleaned = num_cleaned + 1\n",
    "        \n",
    "    return num_cleaned\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do one simulation run with a simple randomized agent that has enough energy for 20 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_environment(simple_randomized_agent, max_steps = 10000,n = 5,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "## General [10 Points]\n",
    "\n",
    "1. Make sure that you use the latest version of this notebook. Sync your forked repository and pull the latest revision. \n",
    "2. Your implementation can use libraries like math, numpy, scipy, but not libraries that implement inteligent agents or complete search algorithms. Try to keep the code simple! In this course, we want to learn about the algorithms and we often do not need to use object-oriented design.\n",
    "3. You notebook needs to be formated professionally. \n",
    "    - Add additional markdown blocks for your description, comments in the code, add tables and use mathplotlib to produce charts where appropriate\n",
    "    - Do not show debugging output or include an excessive amount of output.\n",
    "    - Check that your PDF file is readable. For example, long lines are cut off in the PDF file. You don't have control over page breaks, so do not worry about these.\n",
    "4. Document your code. Add a short discussion of how your implementation works and your design choices.\n",
    "\n",
    "\n",
    "## Task 1: Implement a simulation environment [20 Points]\n",
    "\n",
    "The simple environment above is not very realistic. Your environment simulator needs to follow the PEAS description from above. It needs to:\n",
    "\n",
    "* Initialize the environment by storing the state of each square (clean/dirty) and making some dirty. ([Help with random numbers and arrays in Python](https://github.com/mhahsler/CS7320-AI/blob/master/Python_Code_Examples/random_numbers_and_arrays.ipynb))\n",
    "* Keep track of the agent's position.\n",
    "* Call the agent function repeatedly and provide the agent function with the sensor inputs.  \n",
    "* React to the agent's actions. E.g, by removing dirt from a square or moving the agent around unless there is a wall in the way.\n",
    "* Keep track of the performance measure. That is, track the agent's actions until all dirty squares are clean and count the number of actions it takes the agent to complete the task.\n",
    "\n",
    "The easiest implementation for the environment is to hold an 2-dimensional array to represent if squares are clean or dirty and to call the agent function in a loop until all squares are clean or a predefined number of steps have been reached (i.e., the robot runs out of energy).\n",
    "\n",
    "The simulation environment should be a function like the `simple_environment()` and needs to work with the simple randomized agent program from above. Use the same environmnt for all your agent implementations in the tasks below.\n",
    "\n",
    "*Note on debugging:* Debugging is difficult. Make sure your environment prints enough information when you use `verbose = True`. Also, implementing a function that the environment can use to displays the room with dirt and the current position of the robot at every step is very useful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and description goes here\n",
    "def simple_environment(agent, max_steps, n, verbose = True):\n",
    "    \n",
    "    num_cleaned = 0\n",
    "    sum_energy = 0\n",
    "    \n",
    "    #2-dimensional array to represent environment.\n",
    "    squares = np.zeros((n,n))\n",
    "    \n",
    "    # randomly pick the start position of agent.\n",
    "    agent_x,agent_y = np.random.randint(n),np.random.randint(n)\n",
    "    north,south,west,east = False,False,False,False\n",
    "    \n",
    "    # randomly place dirt.\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            squares[i][j] = np.random.choice([0,1],p=[0.8,0.2])\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(squares)\n",
    "    \n",
    "    # loop until steps have been reached\n",
    "    for i in range(max_steps):\n",
    "        \n",
    "        \n",
    "        if squares[agent_x][agent_y] == 1:\n",
    "            dirty = True\n",
    "        else:\n",
    "            dirty = False\n",
    "            \n",
    "        north,south,west,east = False,False,False,False\n",
    "        if agent_x == 0:\n",
    "            north = True\n",
    "        elif agent_x == n-1:\n",
    "            south = True\n",
    "        \n",
    "        if agent_y == 0:\n",
    "            west = True\n",
    "        elif agent_y == n-1:\n",
    "            east = True\n",
    "\n",
    "        \n",
    "        bumpers = {\"north\" : north, \"south\" : south, \"west\" : west, \"east\" : east}\n",
    "\n",
    "        action = agent(bumpers, dirty)\n",
    "        \n",
    "        # Update agent position accroding to action\n",
    "        if action == \"north\" and north == False:\n",
    "            agent_x -= 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"south\" and south == False:\n",
    "            agent_x += 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"west\" and west == False:\n",
    "            agent_y -= 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"east\" and east == False:\n",
    "            agent_y += 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"suck\" and squares[agent_x][agent_y]==1:\n",
    "            squares[agent_x][agent_y] = 0\n",
    "            num_cleaned += 1\n",
    "            sum_energy += 1\n",
    "            # sum_energy -= 1\n",
    "            if verbose:\n",
    "                print(squares)\n",
    "                \n",
    "        # cannot move towards wall.\n",
    "        else:\n",
    "            # sum_energy -= 1\n",
    "            if verbose:\n",
    "                print(\"Cannot move.\")\n",
    "             \n",
    "        if verbose:\n",
    "            print(\"step\", i , \"- action:\", action) \n",
    "            print(\"enviro_position\",agent_x,agent_y)\n",
    "            \n",
    "        # check wheather all squares are clean\n",
    "        if np.sum(squares) == 0:\n",
    "            if verbose:\n",
    "                print(\"All squares are clean.\")\n",
    "                print(sum_energy)\n",
    "                print(squares)\n",
    "            return sum_energy\n",
    "    if verbose:\n",
    "        print(\"Reached predefined num of steps\")\n",
    "        print(squares)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_environment(simple_reflex_agent, max_steps = 1000, n = 5,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  Implement a simple reflex agent [10 Points] \n",
    "\n",
    "The simple reflex agent randomly walks around but reacts to the bumper sensor by not bumping into the wall and to dirt with sucking. Implement the agent program as a function.\n",
    "\n",
    "_Note:_ Agents cannot directly use variable in the environment. They only gets the percepts as the arguments to the agent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and description goes here\n",
    "import numpy as np\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "def simple_reflex_agent(bumpers, dirty):\n",
    "    \n",
    "    reflex = []\n",
    "    if dirty:\n",
    "        reflex.append(actions[-1])\n",
    "    else:\n",
    "        for key,value in bumpers.items():\n",
    "            if not value:\n",
    "                reflex.append(key)\n",
    "  \n",
    "    \n",
    "    return np.random.choice(reflex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement a model-based reflex agent [20 Points]\n",
    "\n",
    "Model-based agents use a state to keep track of what they have done and perceived so far. Your agent needs to find out where it is located and then keep track of its current location. You also need a set of rules based on the state and the percepts to make sure that the agent will clean the whole room. For example, the agent can move to a corner to determine its location and then it can navigate through the whole room and clean dirty squares.\n",
    "\n",
    "Describe how you define the __agent state__ and how your agent works before implementing it. ([Help with implementing state information on Python](https://github.com/mhahsler/CS7320-AI/blob/master/Python_Code_Examples/store_agent_state_information.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your short description of the state and your implementation goes here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The agent walks from the starting position (randomly generated) north to the head and west to the head. Now we can be sure that the agent's state is in the top left corner. Then start to move with the east-west direction, slowly south, to clean all the squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import numpy as np\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "# is the agent arrive to the northwest corner\n",
    "is_to_corner = False\n",
    "# is the row finished cleaning\n",
    "is_row_finished = False\n",
    "\n",
    "# reflex this action\n",
    "reflex = None\n",
    "# moving reflex before\n",
    "advance = None\n",
    "\n",
    "position = [0,0]\n",
    "\n",
    "\n",
    "def model_reflex_agent(bumpers, dirty):\n",
    "    \n",
    "    global is_to_corner\n",
    "    global is_row_finished\n",
    "    global advance\n",
    "    global reflex\n",
    "    global position\n",
    "    \n",
    "    if not is_to_corner:\n",
    "        reflexes = []\n",
    "        \n",
    "        if not bumpers[\"north\"]:\n",
    "            reflexes.append(actions[0])\n",
    "        if not bumpers[\"west\"]:\n",
    "            reflexes.append(actions[2])\n",
    "        \n",
    "        if len(reflexes) == 0:\n",
    "            if dirty:\n",
    "                reflex = actions[-1]\n",
    "            else:\n",
    "                is_to_corner = True\n",
    "                advance = actions[1]\n",
    "                reflex = advance\n",
    "                position[1] += 1\n",
    "        else:\n",
    "            reflex = np.random.choice(reflexes)\n",
    "    else:\n",
    "        if dirty:\n",
    "            # suck\n",
    "            # print(\"dirty\")\n",
    "            reflex = actions[-1]\n",
    "        else:\n",
    "            if is_row_finished:\n",
    "                if bumpers[\"west\"]:\n",
    "                    advance = actions[1]\n",
    "                elif bumpers[\"east\"]:\n",
    "                    advance = actions[2]\n",
    "                is_row_finished = False\n",
    "            else: \n",
    "                if bumpers[\"west\"] or bumpers[\"east\"]:\n",
    "                    if bumpers[\"south\"]:\n",
    "                        return \n",
    "                    advance = actions[3]\n",
    "                    position[0] += 1\n",
    "                    is_row_finished = True\n",
    "                    \n",
    "            reflex = advance\n",
    "            \n",
    "        if reflex == actions[1]:\n",
    "            position[1] += 1\n",
    "          \n",
    "        elif reflex == actions[2]:\n",
    "            position[1] -= 1\n",
    "        \n",
    "    return reflex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Simulation study [30 Points]\n",
    "\n",
    "Compare the performance (the performance measure is defined in the PEAS description above) of the agents using  environments of different size. E.g., $5 \\times 5$, $10 \\times 10$ and\n",
    "$100 \\times 100$. Use 100 random runs for each. Present the results using tables and graphs. Discuss the differences between the agents. \n",
    "([Help with charts and tables in Python](https://github.com/mhahsler/CS7320-AI/blob/master/Python_Code_Examples/charts_and_tables.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "\n",
    "random_agent,simple_reflex,model_reflex = [],[],[]\n",
    "\n",
    "def test_performance(n):\n",
    "    global random_agent\n",
    "    global simple_reflex\n",
    "    global model_reflex\n",
    "    global is_to_corner\n",
    "    global is_row_finished\n",
    "    global advance\n",
    "    global reflex\n",
    "    global position\n",
    "    \n",
    "\n",
    "    # initialize\n",
    "    max_steps = 100000\n",
    "    random_agent,simple_reflex,model_reflex = [],[],[]\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        # initialize\n",
    "        actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "        is_to_corner = False\n",
    "        is_row_finished = False\n",
    "        reflex = None\n",
    "        advance = None\n",
    "        position = [0,0]\n",
    "        \n",
    "        # seperately call three agents\n",
    "        random_agent.append(simple_environment(simple_randomized_agent, max_steps,n,verbose = False))\n",
    "        simple_reflex.append(simple_environment(simple_reflex_agent, max_steps,n,verbose = False))\n",
    "        model_reflex.append(simple_environment(model_reflex_agent, max_steps,n,verbose = False))\n",
    "        \n",
    "    return [mean(random_agent),mean(simple_reflex),mean(model_reflex)]\n",
    "\n",
    "\n",
    "\n",
    "def showframe(random_agent,simple_reflex,model_reflex):\n",
    "    d = {\n",
    "        \"Randomized Agent\":random_agent,\n",
    "        \"Simple Reflex Agent\":simple_reflex,\n",
    "        \"Model-based Reflex Agent\":model_reflex\n",
    "    }\n",
    "    df = pd.DataFrame(data = d)\n",
    "    display(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283.09, 102.18, 28.75]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Randomized Agent</th>\n",
       "      <th>Simple Reflex Agent</th>\n",
       "      <th>Model-based Reflex Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>355</td>\n",
       "      <td>119</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>82</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>336</td>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604</td>\n",
       "      <td>109</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>383</td>\n",
       "      <td>118</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>286</td>\n",
       "      <td>142</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>326</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>230</td>\n",
       "      <td>121</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>285</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Randomized Agent  Simple Reflex Agent  Model-based Reflex Agent\n",
       "0                355                  119                        25\n",
       "1                116                   82                        33\n",
       "2                336                   74                        35\n",
       "3                604                  109                        27\n",
       "4                408                   37                        39\n",
       "..               ...                  ...                       ...\n",
       "95               383                  118                        31\n",
       "96               286                  142                        31\n",
       "97               326                   21                        29\n",
       "98               230                  121                        37\n",
       "99               285                   69                        32\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2108.42, 910.9, 124.41]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Randomized Agent</th>\n",
       "      <th>Simple Reflex Agent</th>\n",
       "      <th>Model-based Reflex Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245</td>\n",
       "      <td>1054</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1964</td>\n",
       "      <td>1264</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1664</td>\n",
       "      <td>1642</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2475</td>\n",
       "      <td>2081</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1894</td>\n",
       "      <td>824</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1395</td>\n",
       "      <td>1047</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1358</td>\n",
       "      <td>1211</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2304</td>\n",
       "      <td>1066</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1986</td>\n",
       "      <td>707</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2010</td>\n",
       "      <td>839</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Randomized Agent  Simple Reflex Agent  Model-based Reflex Agent\n",
       "0               1245                 1054                       118\n",
       "1               1964                 1264                       111\n",
       "2               1664                 1642                       129\n",
       "3               2475                 2081                       107\n",
       "4               1894                  824                       123\n",
       "..               ...                  ...                       ...\n",
       "95              1395                 1047                       132\n",
       "96              1358                 1211                       118\n",
       "97              2304                 1066                       134\n",
       "98              1986                  707                       124\n",
       "99              2010                  839                       132\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 12096.29]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Randomized Agent</th>\n",
       "      <th>Simple Reflex Agent</th>\n",
       "      <th>Model-based Reflex Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Randomized Agent  Simple Reflex Agent  Model-based Reflex Agent\n",
       "0                 -1                   -1                     12079\n",
       "1                 -1                   -1                     12124\n",
       "2                 -1                   -1                     12083\n",
       "3                 -1                   -1                     12158\n",
       "4                 -1                   -1                     12052\n",
       "..               ...                  ...                       ...\n",
       "95                -1                   -1                     12151\n",
       "96                -1                   -1                     12095\n",
       "97                -1                   -1                     12098\n",
       "98                -1                   -1                     12075\n",
       "99                -1                   -1                     12099\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_5 = test_performance(5)\n",
    "agent1_5,agent2_5,agent3_5 = random_agent,simple_reflex,model_reflex\n",
    "\n",
    "print(mean_5)\n",
    "showframe(random_agent,simple_reflex,model_reflex)\n",
    "\n",
    "mean_10 = test_performance(10)\n",
    "agent1_10,agent2_10,agent3_10 = random_agent,simple_reflex,model_reflex\n",
    "\n",
    "print(mean_10)\n",
    "showframe(random_agent,simple_reflex,model_reflex)\n",
    "\n",
    "print(test_performance(100))\n",
    "showframe(random_agent,simple_reflex,model_reflex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the following table with the average performance measure for 100 random runs (you may also create this table with code):\n",
    "\n",
    "| Size     | Randomized Agent | Simple Reflex Agent | Model-based Reflex Agent |\n",
    "|----------|------------------|---------------------|--------------------------|\n",
    "| 5x5     | 283.09| 102.18| 28.75|\n",
    "| 10x10   | 2108.42| 910.9| 124.41|\n",
    "| 100x100 | >100000| >100000| 12096.29|\n",
    "\n",
    "Add charts to compare the performance of the different agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283.09, 102.18, 28.75]\n",
      "Mean performance(n=5):[283.09, 102.18, 28.75]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance(n=10)[2108.42, 910.9, 124.41]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbY0lEQVR4nO3de5RU5Z3u8e8TQC4CQRFnCK0Bz6CCN8RGcHARxsQLaIKIyfI2XjByTGCG6IxZaOYcYZmVeKLJUSYGhqVM4i06UeOgh2DCeJkZA0ZUVLRR8TLSito6QSAC0vI7f+zdWDa7u4uu3l3d1c9nrV5dtS+137eAenjfvfevFBGYmZk19rlyN8DMzDomB4SZmWVyQJiZWSYHhJmZZXJAmJlZpu7lbkBb2m+//WLo0KHlboaZWafx1FNPvR8Rg7LWVVRADB06lFWrVpW7GWZmnYak/2pqnaeYzMwskwPCzMwyOSDMzCxTRZ2DMLOuaceOHdTW1rJt27ZyN6XD6tWrF1VVVfTo0aPofRwQZtbp1dbW0q9fP4YOHYqkcjenw4kIPvjgA2praxk2bFjR+3mKycw6vW3btjFw4ECHQxMkMXDgwD0eYTkgzKwiOBya15r3xwFhZmaZfA7CzCrOlfc936av98MzjmjT12uNjRs3cuedd/Ltb387c/306dN58MEH2X///VmzZk2bHNMjiCLNWzGv3E0wsy5s48aN/OxnP2ty/YUXXsiyZcva9JgOCDOzNnD66adzzDHHcNhhh7Fo0aJdy2+55RYOPvhgJk6cyCWXXMKsWbMAqKurY9q0aYwZM4YxY8bw+OOPAzB37lymT5/OxIkTOeigg5g/fz4Ac+bM4dVXX2XUqFFcccUVux1/woQJ7Lvvvm3aJ08xmZm1gcWLF7PvvvuydetWxowZw7Rp09i+fTvXXHMNTz/9NP369eOEE07gqKOOAmD27NlcdtllHH/88bz55pucfPLJ1NTUALB27VoeeeQRNm/ezCGHHMK3vvUtrr32WtasWcPq1avbrU8OCDOzNjB//nx+/etfA7B+/XpeeeUV3nnnHb70pS/t+p/917/+dV5++WUAli9fzosvvrhr/02bNrF582YATj31VHr27EnPnj3Zf//9effdd9u5NwkHhJlZiR599FGWL1/OihUr6NOnDxMnTmTbtm1ERJP77Ny5kxUrVtC7d+/d1vXs2XPX427dulFfX59Lu1vicxBmZiX68MMP2WeffejTpw9r165l5cqVABx77LE89thj/PGPf6S+vp5777131z4nnXQSP/3pT3c9b2nqqF+/frtGGO3FIwgzqzjtfVnqKaecwsKFCznyyCM55JBDGDduHABDhgzhqquuYuzYsXzhC19g5MiRfP7znweSKamZM2dy5JFHUl9fz4QJE1i4cGGTxxg4cCDjx4/n8MMPZ9KkSVx33XWfWX/22Wfz6KOP8v7771NVVcW8efO4+OKLS+qXmhsCdTbV1dWR1xcGzVsxj6uPuzqX1zaz0tTU1DBixIhyNyPTli1b6Nu3L/X19UydOpXp06czderUsrQl632S9FREVGdt7ykmM7MczZ07l1GjRnH44YczbNgwTj/99HI3qWieYjIzy9H1119f7ia0mkcQZmaWyQFhZmaZHBB7wPWYzKwrcUCYmVkmn6Q2s8rzwOy2fb2v3ti2r9cKzZX7Xr9+Peeffz7vvPMOn/vc55gxYwazZ5f+HngEYWbWCTRX7rt79+78+Mc/pqamhpUrV3LTTTd9ps5TazkgzMzaQDnLfQ8ePJjRo0cDSUmOESNG8NZbb5XcJ08xmZm1gY5S7vuNN97gmWeeYezYsSX3KdeAkHQKcCPQDbg5Iq5ttF7p+snAR8CFEfF0uu4y4JtAAM8DF0XEtjzba2bWWh2h3PeWLVuYNm0aN9xwA/379y+5T7kFhKRuwE3AiUAt8KSkJRFRODE2CRie/owFFgBjJQ0B/hYYGRFbJf0LcBbw87zaa2bWWh2h3PeOHTuYNm0a5557LmeccUbrOtJInucgjgXWRcRrEfExcBcwpdE2U4BbI7ESGCBpcLquO9BbUnegD/B2jm01M2u1cpf7jgguvvhiRowYweWXX156h1J5TjENAdYXPK8lGSW0tM2QiFgl6XrgTWAr8NuI+G2ObTWzStLOl6WWu9z3448/zm233cYRRxzBqFGjAPjBD37A5MmTS+pXngGhjGWNx1uZ20jah2R0MQzYCPxK0nkRcftuB5FmADMADjzwwJIabGbWGj179uQ3v/lN5rpzzjmHGTNm7Cr3fdJJJwGw3377cffdd++2/dy5cz/zfM2aNbse33nnnZnHOP7445udzmqtPKeYaoEDCp5Xsfs0UVPbfAV4PSLqImIHcB/wl1kHiYhFEVEdEdWDBg1qs8abmbUFl/vO9iQwXNIw4C2Sk8znNNpmCTBL0l0k008fRsQGSW8C4yT1IZli+jKQzzcBmZnlqDOX+84tICKiXtIs4CGSy1wXR8QLki5N1y8ElpJc4rqO5DLXi9J1T0i6B3gaqAeeARbtfhQzM8tLrvdBRMRSkhAoXLaw4HEAM5vY92rA3/FpZlYmLrVhZmaZHBBmZpbJtZjMrOK09Zd7XX1c+We7myv3vW3bNiZMmMD27dupr6/nzDPPZN680t8DjyDMzDqB5sp99+zZk4cffphnn32W1atXs2zZsl13c5fCAWFm1gbKWe5bEn379gWSmkw7duwgqYVaGk8xmZm1gXKX+/7kk0845phjWLduHTNnzuz45b7NzLqKcpf77tatG6tXr2bjxo1MnTqVNWvWcPjhh5fUJweEmVmJOkK57wYDBgxg4sSJLFu2rOSA8DkIM7MSlbvcd11dHRs3bgRg69atLF++nEMPPbS0TuERhJlVoPa+LLXc5b43bNjABRdcwCeffMLOnTv5xje+wWmnnVZyv5RHidhyqa6ujlWr8qnp13BddUe4HtrMPqumpoYRI0aUuxmZtmzZQt++fXeV+54+fTpTp04tS1uy3idJT0VEddb2nmIyM8uRy32bmVmmzlzu2yMIM6sIlTRdnofWvD8OCDPr9Hr16sUHH3zgkGhCRPDBBx/Qq1evPdrPU0xm1ulVVVVRW1tLXV1duZvSYfXq1Yuqqqo92scBYWadXo8ePRg2bFi5m1FxPMVkZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZplyDQhJp0h6SdI6SXMy1kvS/HT9c5JGF6wbIOkeSWsl1Ug6Ls+2mpnZZ+UWEJK6ATcBk4CRwNmSRjbabBIwPP2ZASwoWHcjsCwiDgWOAmryaquZme0uzxHEscC6iHgtIj4G7gKmNNpmCnBrJFYCAyQNltQfmADcAhARH0fExhzbamZmjeQZEEOA9QXPa9NlxWxzEFAH/LOkZyTdLGnvrINImiFplaRVdXV1bdd6M7MuLs+AUMayKHKb7sBoYEFEHA38CdjtHAZARCyKiOqIqB40aFAp7TUzswJ5BkQtcEDB8yrg7SK3qQVqI+KJdPk9JIFhZmbtJM+AeBIYLmmYpL2As4AljbZZApyfXs00DvgwIjZExDvAekmHpNt9GXgxx7aamVkj3fN64YiolzQLeAjoBiyOiBckXZquXwgsBSYD64CPgIsKXuJvgDvScHmt0TozM8tZbgEBEBFLSUKgcNnCgscBzGxi39VAdZ7tMzOzpvlOajMzy+SAMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy+SAMDOzTEUFhKR7JZ0qyYFiZtZFFPuBvwA4B3hF0rWSDs2xTWZm1gEUFRARsTwiziWpqPoG8DtJv5d0kaQeeTbQzMzKo+gpI0kDgQuBbwLPkHwl6Gjgd7m0zMzMyqqoYn2S7gMOBW4DvhoRG9JVd0talVfjzMysfIqt5npzWpl1F0k9I2J7RLjiqplZBSp2iun7GctWtGVDzMysY2l2BCHpz4EhQG9JR/Ppd0j3B/rk3DYzMyujlqaYTiY5MV0F/KRg+WbgqpzaZGZmHUCzARERvwB+IWlaRNzbTm0yM7MOoKUppvMi4nZgqKTLG6+PiJ9k7GZmZhWgpSmmvdPfffNuiJmZdSwtTTH9U/p7Xvs0x8zMOopii/X9SFJ/ST0k/Zuk9yWdl3fjzMysfIq9D+KkiNgEnAbUAgcDV+TWKjMzK7tiA6KhIN9k4JcR8d85tcfMzDqIYkttPCBpLbAV+LakQcC2/JplZmblVmy57znAcUB1ROwA/gRMybNhZmZWXsWOIABGkNwPUbjPrW3cHjMz6yCKLfd9G/A/gNXAJ+niwAFhZlaxih1BVAMjIyLybIyZmXUcxV7FtAb48zwbYmZmHUuxI4j9gBcl/QHY3rAwIr6WS6vMzKzsig2IuXk2wszMOp6iAiIiHpP0RWB4RCyX1Afolm/TzMysnIqtxXQJcA/wT+miIcD9ObXJzMw6gGJPUs8ExgObACLiFWD/vBplZmblV2xAbI+IjxuepDfL+ZJXM7MKVmxAPCbpKqC3pBOBXwEP5NcsMzMrt2IDYg5QBzwP/E9gKfAPLe0k6RRJL0laJ2lOxnpJmp+uf07S6Ebru0l6RtKDRbbTzMzaSLFXMe2UdD9wf0TUFbOPpG7ATcCJJN8h8aSkJRHxYsFmk4Dh6c9YYEH6u8FsoAboX8wxzcys7TQ7gkj/hz9X0vvAWuAlSXWS/ncRr30ssC4iXkvPX9zF7hVgpwC3RmIlMEDS4PTYVcCpwM172CczM2sDLU0xfYfk6qUxETEwIvYl+R/+eEmXtbDvEGB9wfPadFmx29wAfBfY2dxBJM2QtErSqrq6ogY3ZmZWhJYC4nzg7Ih4vWFBRLwGnJeua44yljW+8ilzG0mnAe9FxFMtHIOIWBQR1RFRPWjQoJY2NzOzIrUUED0i4v3GC9PzED0yti9UCxxQ8LwKeLvIbcYDX5P0BsnU1AmSbm/heGZm1oZaCoiPW7kO4ElguKRhkvYCzgKWNNpmCXB+eq5jHPBhRGyIiCsjoioihqb7PRwR57VwPDMza0MtXcV0lKRNGcsF9Gpux4iolzQLeIikbtPiiHhB0qXp+oUkl8tOBtYBHwEX7WH7zcwsJ80GRESUVJAvIpaShEDhsoUFj4OkjEdzr/Eo8Ggp7TAzsz1X7I1yZmbWxTggzMwskwOiCPNWzMt8bGZWyRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskB0QzXXTKzrswBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEC1wPSYz66ocEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZcg0ISadIeknSOklzMtZL0vx0/XOSRqfLD5D0iKQaSS9Imp1nO83MbHe5BYSkbsBNwCRgJHC2pJGNNpsEDE9/ZgAL0uX1wN9FxAhgHDAzY18zM8tRniOIY4F1EfFaRHwM3AVMabTNFODWSKwEBkgaHBEbIuJpgIjYDNQAQ3Jsq5mZNZJnQAwB1hc8r2X3D/kWt5E0FDgaeKLtm2hmZk3JMyCUsSz2ZBtJfYF7ge9ExKbMg0gzJK2StKqurq7VjTUzs8/KMyBqgQMKnlcBbxe7jaQeJOFwR0Tc19RBImJRRFRHRPWgQYPapOFmZpZvQDwJDJc0TNJewFnAkkbbLAHOT69mGgd8GBEbJAm4BaiJiJ/k2EYzM2tC97xeOCLqJc0CHgK6AYsj4gVJl6brFwJLgcnAOuAj4KJ09/HAXwPPS1qdLrsqIpbm1V4zM/us3AICIP1AX9po2cKCxwHMzNjvP8k+P2FmZu3Ed1KbmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWaZcvw+iS3pgdtPrvnpj+7XDzKxEHkE0Yd6KeeVugplZWTkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMvk+iPbkeyTMrBPxCMLMzDI5IMzMLJOnmDoKTz+ZWQfjEYSZmWXyCKI1mvvfvplZhXBAFHCBPjOzT3mKyczMMjkgzMwskwPCzMwy+RxEU/7r902v2/vQ9msH+BJYMysLB0QFuvK+58vdhD32wzOOKHcTzKwRTzGZmVkmjyA6qCde/++itru/E44WspRj1ONRi1nzHBAV6vTaHzW57v6q77ZjS8yss8o1ICSdAtwIdANujohrG61Xun4y8BFwYUQ8Xcy+nUGxo4BSNBcE1rxSRi0efVhXkFtASOoG3AScCNQCT0paEhEvFmw2CRie/owFFgBji9y3Xby3aftuy554L/8P/jx5dGFmxchzBHEssC4iXgOQdBcwBSj8kJ8C3BoRAayUNEDSYGBoEftaF9beIdf4eE/Mb/3xShl9lOsKNY+Y8tcRR7R5BsQQYH3B81qSUUJL2wwpcl8AJM0AZqRPt0h6qZXt3Q94v5gNf9bKA3QARfTx9nZpSKman2+8veg/yzY6Xhu+1h5p8342pczzu+3WzzJrdT9L/PP5YlMr8gwIZSyLIrcpZt9kYcQiYNGeNW13klZFRHWpr9ORdYU+gvtZadzP8skzIGqBAwqeVwFvF7nNXkXsa2ZmOcrzRrkngeGShknaCzgLWNJomyXA+UqMAz6MiA1F7mtmZjnKbQQREfWSZgEPkVyqujgiXpB0abp+IbCU5BLXdSSXuV7U3L55tTVV8jRVJ9AV+gjuZ6VxP8tEyQVEZmZmn+VaTGZmlskBYWZmmbp8QEg6RdJLktZJmlPu9pRC0gGSHpFUI+kFSbPT5ftK+p2kV9Lf+xTsc2Xa95cknVy+1u8ZSd0kPSPpwfR5xfURIL159B5Ja9M/1+Mqra+SLkv/vq6R9EtJvSqlj5IWS3pP0pqCZXvcN0nHSHo+XTc/LVOUv4josj8kJ8BfBQ4iubT2WWBkudtVQn8GA6PTx/2Al4GRwI+AOenyOcD/SR+PTPvcExiWvhfdyt2PIvt6OXAn8GD6vOL6mLb/F8A308d7AQMqqa8kN8W+DvROn/8LcGGl9BGYAIwG1hQs2+O+AX8AjiO5R+w3wKT2aH9XH0HsKgcSER8DDSU9OqWI2BBpscOI2AzUkPwDnELyQUP6+/T08RTgrojYHhGvk1xNdmy7NroVJFUBpwI3FyyuqD4CSOpP8gFzC0BEfBwRG6m8vnYHekvqDvQhueepIvoYEf8ONC7etkd9S8sP9Y+IFZGkxa0F++SqqwdEU6U+Oj1JQ4GjgSeAP4vk/hLS3/unm3XW/t8AfBfYWbCs0voIyci2DvjndDrtZkl7U0F9jYi3gOuBN4ENJPdC/ZYK6mOGPe3bkPRx4+W56+oBUXRJj85EUl/gXuA7EbGpuU0zlnXo/ks6DXgvIp4qdpeMZR26jwW6k0xPLIiIo4E/kUxJNKXT9TWdf59CMqXyBWBvSec1t0vGsg7dxz1QcumhttbVA6KYciCdiqQeJOFwR0Tcly5+Nx2mkv5+L13eGfs/HviapDdIpgRPkHQ7ldXHBrVAbUQ8kT6/hyQwKqmvXwFej4i6iNgB3Af8JZXVx8b2tG+16ePGy3PX1QOiokp6pFc23ALURMRPClYtAS5IH18A/GvB8rMk9ZQ0jOR7Of7QXu1tjYi4MiKqImIoyZ/XwxFxHhXUxwYR8Q6wXtIh6aIvk5S8r6S+vgmMk9Qn/fv7ZZJzZ5XUx8b2qG/pNNRmSePS9+j8gn3yVe6z/OX+ISn18TLJFQPfK3d7SuzL8SRDz+eA1enPZGAg8G/AK+nvfQv2+V7a95dopysj2rC/E/n0KqZK7eMoYFX6Z3o/sE+l9RWYB6wF1gC3kVzFUxF9BH5Jcm5lB8lI4OLW9A2oTt+fV4GfklbByPvHpTbMzCxTV59iMjOzJjggzMwskwPCzMwyOSDMzCyTA8LMzDI5IKzTkfSJpNVp9c9fSeqzh/tfl1YPvS6vNuYpre56UBu8TsP7uFrSkoLld0kaXurrW+fny1yt05G0JSL6po/vAJ6Kz94Y2NR+3SP5OttNwKCI2F7k8bpHRH1prW4bkg4Dvh8RU9vgtXa9j42Wfwk4LyIuKfUY1rnl9p3UZu3kP4Aj0yJ2/wgcQfL3em5E/KukC0kqv/YiqfOzBdgbeELSD4GVwGJgEElhvIsi4k1JPyepwnk08LSkgcBW4FDgiyTfn34BSQnmJyLiQgBJC4AxQG/gnoi4Ol3+Bknlzq8CPYCvR8TatG7WP5LcCBXAvIi4V9JJJDeQ9SS5OeqiiNgCnEvBXbRpf24ETkvbNyUi3m2D9/TnHSkYrTw8xWSdVloeehLwPMkdqA9HxBjgr4Dr0tCA5EP8gog4ISK+BmyNiFERcTfJXam3RsSRwB3A/IJDHAx8JSL+Ln2+D3ACcBnwAPB/gcOAIySNSrf5XkRUA0cCX5J0ZMHrvR8Ro4EFwN+ny/4XSQXTI9I2PCxpP+Af0mOPJrmT+vJ0+/FAYaHCvYGVEXEU8O/AJel7c27B9FHhzz0F+/aStErSSkmnNyyMiJ0kpaaPauq9t67BIwjrjHpLWp0+/g+S+lO/Jyni1/DB2ws4MH38u4hoXJO/wXHAGenj20i+zKXBryLik4LnD0RESHoeeDcingeQ9AIwlKS0yTckzSD5tzWY5Etgnkv3byie+FTBMb9CUlMKgIj4Y1qxdiTwePrFYXsBK9JNBpOMdBp8DDxY8Lonpq9zB0ngNefAiHg7PZ/xsKTnI+LVdN17JNVVi62aaxXIAWGd0daIGFW4IC1iNi0iXmq0fCxJmexiFZ6Ua7xfwzmLnQWPG553Twus/T0wJv2g/zlJUDXe/xM+/bcndi/dLJJQOzujfVsbveaO+PRE4q7XlXQucEXG/usi4kyAiHg7/f2apEdJptMaAqJXeizrwjzFZJXiIeBvGr6rV9LRRe73ez79H/y5wH+W0Ib+JKHyoaQ/I5n+aslvgVkNT9LvR1gJjJf0F+myPpIOTjepAf6ipReNiDvSabTGP2c2HEdSz/TxfiRTVy8WvMTBwAtFtN8qmAPCKsU1JCd/n1PyBfHXFLnf3wIXSXoO+GtgdmsbEBHPAs+QfLAuBh4vYrfvA/ukl+w+C/xVRNSRfC/zL9N2rSQ5OQ7w/0iq2JZqBLAqPeYjwLUR8SJAGm5bI/3WM+u6fJmrWSciqTfJB/r4RudH2vIYlwGbIuKWPF7fOg+PIMw6kYjYClxNvt9JvJHkklzr4jyCMDOzTB5BmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWab/Dylqi0tDyZZcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your graphs and discussion of the results goes here\n",
    "x_label = [\"agent1\",\"agent2\",\"agent3\"]\n",
    "print(mean_5)\n",
    "\n",
    "print(f\"Mean performance(n=5):{mean_5}\")\n",
    "\n",
    "plt.bar(x_label,mean_5)\n",
    "plt.ylabel(\"Mean performance\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean performance(n=10){mean_10}\")\n",
    "plt.bar(x_label,mean_5)\n",
    "plt.ylabel(\"Mean performance\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(agent1_5, bins = 20, alpha = .6, density=True)\n",
    "plt.hist(agent2_5, bins = 20, alpha = .6, density=True)\n",
    "plt.hist(agent3_5, bins = 20, alpha = .6, density=True)\n",
    "\n",
    "plt.xlabel(\"Performance(n=5)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(labels = [\"agent 1\", \"agent 2\", \"agent 3\"])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(agent1_10, bins = 20, alpha = .6, density=True)\n",
    "plt.hist(agent2_10, bins = 20, alpha = .6, density=True)\n",
    "plt.hist(agent3_10, bins = 20, alpha = .6, density=True)\n",
    "\n",
    "plt.xlabel(\"Performance(n=10)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(labels = [\"agent 1\", \"agent 2\", \"agent 3\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Randomized Agent consumes the most energy, more than twice as much as the Simple Reflex Agent and ten to twenty times as much as the Model-based Reflex Agent, by averaging the energy used on three different floor sizes. The Model-based Reflex energy used also grows the slowest. It grows linearly with the increase in area.\n",
    "\n",
    "The histogram of all the test results shows that the Model-based Reflex Agent not only uses the least amount of energy, but also the results are very concentrated and the total amount of energy used each time is very stable. The results of the other two Agents are very scattered and it is difficult to predict and control the total amount of energy used each time. The Randomized Agent has the most scattered distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Robustness of the agent implementations [10 Points] \n",
    "\n",
    "Describe how your agent implementations will perform \n",
    "\n",
    "* if it is put into a rectangular room with unknown size, \n",
    "* if the cleaning area can have an iregular shape (e.g., a hallway connecting two rooms), or \n",
    "* if the room contains obstacles (i.e., squares that it cannot pass through and trigger the bumper sensors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer goes here\n",
    "##### If put the agent into a rectangular room with unknown size: \n",
    "- Simple randomized agent will complete the cleaning successfully\n",
    "- Simple reflex agent will complete the cleaning successfully\n",
    "- Model based agent will complete the cleaning successfully\n",
    "\n",
    "##### If the cleaning area has an iregular shape: \n",
    "- Simple randomized agent will complete the cleaning successfully \n",
    "- Simple reflex agent will complete the cleaning successfully\n",
    "- Model based agent will cleaning only part of the area\n",
    "\n",
    "##### If the room contains obstacles: \n",
    "- Simple randomized agent will complete the cleaning successfully\n",
    "- Simple reflex agent will complete the cleaning successfully\n",
    "- Model based agent will cleaning only part of the area, or may being stuck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate student advanced task: Obstacles [10 Points]\n",
    "\n",
    "__Undergraduate students:__ This is a bonus task you can attempt if you like [+5 Bonus Points].\n",
    "\n",
    "1. Change your simulation environment tor run experiments for the following problem: Add random obstacle squares that also trigger the bumper sensor. The agent does not know where the obstacles are. Observe how this changes the performance of the three implementations.\n",
    "\n",
    "2. Describe what would need to be done to perform better with obstacles. Add code if you can. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and discussion goes here\n",
    "def simple_environment(agent, max_steps, n, verbose = True):\n",
    "    \n",
    "    num_cleaned = 0\n",
    "    sum_energy = 0\n",
    "    \n",
    "    # 2-dimensional array to represent environment.\n",
    "    squares = np.zeros((n,n))\n",
    "    \n",
    "    north,south,west,east = False,False,False,False\n",
    "    \n",
    "    # randomly place dirt(1) and obstacles(2)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            squares[i][j] = np.random.choice([0,1,2],p=[0.7,0.2,0.1])\n",
    "            \n",
    "    # randomly pick the start position of agent.\n",
    "    agent_x,agent_y = np.random.randint(n),np.random.randint(n)\n",
    "    while squares[agent_x][agent_y] == 2:\n",
    "        print(agent_x,agent_y,\"rechoose\")\n",
    "        agent_x,agent_y = np.random.randint(n),np.random.randint(n)\n",
    "    print(\"Start:\",agent_x,agent_y)\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(squares)\n",
    "    \n",
    "    # loop until steps have been reached\n",
    "    for i in range(max_steps):\n",
    "        \n",
    "        \n",
    "        if squares[agent_x][agent_y] == 1:\n",
    "            dirty = True\n",
    "        else:\n",
    "            dirty = False\n",
    "            \n",
    "        north,south,west,east = False,False,False,False\n",
    "        if agent_x == 0:\n",
    "            north = True\n",
    "        elif agent_x == n-1:\n",
    "            south = True\n",
    "        \n",
    "        if agent_y == 0:\n",
    "            west = True\n",
    "        elif agent_y == n-1:\n",
    "            east = True\n",
    "        \n",
    "        # there will be a wall if squares == 2\n",
    "        if agent_x != 0 and squares[agent_x-1][agent_y] == 2:\n",
    "            north = True\n",
    "        if agent_x != n-1 and squares[agent_x+1][agent_y] == 2:\n",
    "            south = True\n",
    "        if agent_y != 0 and squares[agent_x][agent_y-1] == 2:\n",
    "            west = True\n",
    "        if agent_y != n-1 and squares[agent_x][agent_y+1] == 2:\n",
    "            east = True\n",
    "              \n",
    "        bumpers = {\"north\" : north, \"south\" : south, \"west\" : west, \"east\" : east}\n",
    "\n",
    "        action = agent(bumpers, dirty)\n",
    "        \n",
    "        # Update position info accroding to action\n",
    "        if action == \"north\" and north == False:\n",
    "            agent_x -= 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"south\" and south == False:\n",
    "            agent_x += 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"west\" and west == False:\n",
    "            agent_y -= 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"east\" and east == False:\n",
    "            agent_y += 1\n",
    "            sum_energy += 1\n",
    "        elif action == \"suck\" and squares[agent_x][agent_y]==1:\n",
    "            squares[agent_x][agent_y] = 0\n",
    "            num_cleaned += 1\n",
    "            sum_energy += 1\n",
    "            # sum_energy -= 1\n",
    "            if verbose:\n",
    "                print(squares)\n",
    "                \n",
    "        # cannot move towards wall.\n",
    "        else:\n",
    "            # sum_energy -= 1\n",
    "            if verbose:\n",
    "                print(action)\n",
    "                print(\"Cannot move.\")\n",
    "             \n",
    "        if verbose:\n",
    "            print(\"step\", i , \"- action:\", action) \n",
    "            print(\"enviro_position\",agent_x,agent_y)\n",
    "            \n",
    "        # check wheather all squares are clean\n",
    "        if not (squares == 1).any():\n",
    "            if verbose:\n",
    "                print(\"All squares are clean.\")\n",
    "                print(sum_energy)\n",
    "                print(squares)\n",
    "            return sum_energy\n",
    "    if verbose:\n",
    "        print(\"Reached predefined num of steps\")\n",
    "        print(squares)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Start: 2 1\n",
      "277\n",
      "-----------------------------------\n",
      "Start: 2 1\n",
      "108\n",
      "-----------------------------------\n",
      "3 1 rechoose\n",
      "Start: 3 4\n",
      "-1\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "is_to_corner = False\n",
    "is_row_finished = False\n",
    "reflex = None\n",
    "advance = None\n",
    "position = [0,0]\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(simple_environment(simple_reflex_agent, max_steps = 10000, n = 5,verbose = False))\n",
    "print(\"-----------------------------------\")\n",
    "print(simple_environment(simple_randomized_agent, max_steps = 10000,n = 5,verbose = False))\n",
    "print(\"-----------------------------------\")\n",
    "print(simple_environment(model_reflex_agent, max_steps = 10,n = 5,verbose = False))\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we add random obstacle squares in the room: \n",
    "Simple randomized agent will complete cleaning successfully. \n",
    "Simple reflex agent will complete cleaning successfully. \n",
    "Model based agent will fail to respond to obstacles successfully and cannot finish cleaning\n",
    "\n",
    "##### If we want our model based agent perform better with obstacles: \n",
    "We should add obstacles recognize function in it. Which mean after the agent finish scanning the room can start to clean, if it senses an unexpected bumper in its way, it should bypass the obstacle by moving like south-east-east-north if there is an obstacles on its east. And then it will go back to it expected route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More advanced implementation tasks\n",
    "\n",
    "* __Agent for and environment with obstacles:__ Implement an agent for an environment where the agent does not know how large the environment is (we assume it is rectangular), where it starts or where the obstacles are. An option would be to always move to the closest unchecked/uncleaned square (note that this is actualy depth-first search).\n",
    "\n",
    "* __Utility-based agent:__ Change the environment for a $5 \\times 5$ room, so each square has a fixed probability of getting dirty again. For the implementation, we give the environment a 2-dimensional array of probabilities. The utility of a state is defined as the number of currebntly clean squares in the room. Implement a utility-based agent that maximizes the expected utility over one full charge which lasts for 100000 time steps. To do this, the agent needs to learn the probabilities with which different squares get dirty again. This is very tricky!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "##### Unknown environment with obstacles\n",
    "Record each square's state in the agent, before each step, scan the squares around the agent which are one step away from the agent. If there is an unchecked or uncleaned square, agent goes to it. If there isn't an one step square, then search all the two step away square. If yes, go to it. If no, then search three step squares...\n",
    "\n",
    "##### Utility-based agent\n",
    "Using the function that dealing with unknown environment with obstacles above to always go to the nearest dirty sqaure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
